@article{Luan2018,
abstract = {Copying an element from a photo and pasting it into a painting is a challenging task. Applying photo compositing techniques in this context yields subpar results that look like a collage --- and existing painterly stylization algorithms, which are global, perform poorly when applied locally. We address these issues with a dedicated algorithm that carefully determines the local statistics to be transferred. We ensure both spatial and inter-scale statistical consistency and demonstrate that both aspects are key to generating quality results. To cope with the diversity of abstraction levels and types of paintings, we introduce a technique to adjust the parameters of the transfer depending on the painting. We show that our algorithm produces significantly better results than photo compositing or global stylization techniques and that it enables creative painterly edits that would be otherwise difficult to achieve.},
archivePrefix = {arXiv},
arxivId = {1804.03189},
author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
eprint = {1804.03189},
file = {::},
month = {apr},
title = {{Deep Painterly Harmonization}},
url = {http://arxiv.org/abs/1804.03189},
year = {2018}
}
@article{Shen2017,
abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of {\$}4.53{\$} comparable to a MOS of {\$}4.58{\$} for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and {\$}F{\_}0{\$} features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.},
archivePrefix = {arXiv},
arxivId = {1712.05884},
author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, RJ and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
eprint = {1712.05884},
file = {::},
month = {dec},
title = {{Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions}},
url = {http://arxiv.org/abs/1712.05884},
year = {2017}
}
@misc{Openai2017,
abstract = {Our Dota 2 result shows that self-play can catapult the performance of machine learning systems from far below human level to superhuman, given sufficient compute. In the span of a month, our system went from barely matching a high-ranked player to beating the top pros and has continued to improve since then. Supervised deep learning systems can only be as good as their training datasets, but in self-play systems, the available data improves automatically as the agent gets better.},
author = {Openai},
booktitle = {Blog},
title = {{More on Dota 2}},
url = {https://blog.openai.com/more-on-dota-2/},
urldate = {2018-05-19},
year = {2017}
}
@article{Searle1980,
author = {Searle, John R.},
doi = {10.1017/S0140525X00005756},
file = {::},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {artificial intelligence,brain,intentionality,mind},
month = {sep},
number = {03},
pages = {417},
publisher = {Cambridge University Press},
title = {{Minds, brains, and programs}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X00005756},
volume = {3},
year = {1980}
}
@article{moor2006dartmouth,
author = {Moor, James},
journal = {Ai Magazine},
number = {4},
pages = {87},
title = {{The Dartmouth College artificial intelligence conference: The next fifty years}},
volume = {27},
year = {2006}
}
@misc{Marr2018,
abstract = {Discussions of artificial intelligence (AI) have created a certain amount of unease by those who fear it will quickly evolve from being a benefit to human society to taking over. Even Stephen Hawking and Elon Musk have warned of AI's threats. However, we're not all operating from the same definition of the term and while the foundation is generally the same, the focus of artificial intelligence shifts depending on the entity that provides the definition.},
author = {Marr, Bernard},
booktitle = {forbes},
title = {{The Key Definitions Of Artificial Intelligence (AI) That Explain Its Importance}},
url = {https://www.forbes.com/sites/bernardmarr/2018/02/14/the-key-definitions-of-artificial-intelligence-ai-that-explain-its-importance/{\#}46d5bcdc4f5d},
urldate = {2018-05-13},
year = {2018}
}
@book{McCarthy1956,
author = {McCarthy, John and Shannon, Claude Elwood},
isbn = {9780691079165},
pages = {285},
publisher = {Princeton University Press},
title = {{Automata Studies: Annals of Mathematics Studies. Number 34 - William Ross Ashby}},
year = {1956}
}
@article{Feng2018,
abstract = {We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment. To achieve this, we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space, then train a simple Convolutional Neural Network to regress it from a single 2D image. We also integrate a weight mask into the loss function during training to improve the performance of the network. Our method does not rely on any prior face model, and can reconstruct full facial geometry along with semantic meaning. Meanwhile, our network is very light-weighted and spends only 9.8ms to process an image, which is extremely faster than previous works. Experiments on multiple challenging datasets show that our method surpasses other state-of-the-art methods on both reconstruction and alignment tasks by a large margin.},
archivePrefix = {arXiv},
arxivId = {1803.07835},
author = {Feng, Yao and Wu, Fan and Shao, Xiaohu and Wang, Yanfeng and Zhou, Xi},
eprint = {1803.07835},
file = {::},
month = {mar},
title = {{Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network}},
url = {http://arxiv.org/abs/1803.07835},
year = {2018}
}
@misc{Champandard2012,
abstract = {When you use traditional terrain generation algorithms, the results you get are mostly lifeless. Throwing in a few chicken here and there helps, but it doesn't give you that fascinating world to explore with its own history where everything you see has a reason.},
author = {Champandard, Alex J.},
booktitle = {AiGameDev},
title = {{Beyond Terrain Generation: Bringing Worlds in DWARF FORTRESS to Life | AiGameDev.com}},
url = {https://aigamedev.com/open/teaser/living-worlds-dwarf-fortress/},
urldate = {2018-05-13},
year = {2012}
}
@misc{Stanton2013,
abstract = {Nothing is as devastating as making a deep run in Spelunky and losing it all to a stupid risk. Rogue Legacy has something of the same but is more forgiving, thanks mainly to hundreds of incremental but permanent player upgrades.},
author = {Stanton, Rich},
booktitle = {Eurogamer},
title = {{The making of Rogue Legacy • Eurogamer.net}},
url = {https://www.eurogamer.net/articles/2013-07-29-the-making-of-rogue-legacy},
urldate = {2018-05-13},
year = {2013}
}
@misc{Bertz2011,
abstract = {Many of Fallout's technological refinements carry over to The Elder Scrolls V: Skyrim, but Bethesda Studios has also developed and contracted a suite of technological tools that allow the team to reach far beyond anything they've done before.},
author = {Bertz, Matt},
booktitle = {Gameinformer},
pages = {2},
title = {{The Technology Behind The Elder Scrolls V: Skyrim}},
url = {https://web.archive.org/web/20111229030542/http://www.gameinformer.com/games/the{\_}elder{\_}scrolls{\_}v{\_}skyrim/b/xbox360/archive/2011/01/17/the-technology-behind-elder-scrolls-v-skyrim.aspx?PostPageIndex=2},
urldate = {2018-05-13},
year = {2011}
}
@misc{PCGamerCarterInterview,
abstract = {A shadow has been cast over the imperial province of Cyrodiil. The emperor has been assassinated by unknown forces. The gates to Oblivion have been opened and hellish Daedra pour out to ravage the land. At the center of it all? You. A doomed prisoner set free by chance. Will you use your second chance at life to save Cyrodiil from a terrible fate, or answer the call of another destiny? In this game, you are sent fourth to try and find the last remaining heir to the throne who is hidden because he is illegitimate, yet still possesses the necessary blood to restore order to the land. While this is the main story, the game is incredibly open in that there are many villages, all with quests that need not relate to the primary goals.},
author = {{Bethesda Game Studios}},
booktitle = {PCGamer},
isbn = {B003HFFSIE},
pages = {1st, 3rd Person},
title = {{The Elder Scrolls 5: Skyrim}},
url = {http://www.elderscrolls.com/games/oblivion{\_}overview.htm},
urldate = {2018-05-11},
year = {2011}
}
@misc{EurogamerMiyamotoInterview,
abstract = {Miyamoto on World 1-1: How Nintendo made Mario's most iconic level},
author = {Eurogamer},
booktitle = {Eurogamer},
title = {{Miyamoto on World 1-1}},
url = {https://www.youtube.com/watch?v=zRGRJRUWafY},
urldate = {2018-05-11},
year = {2015}
}
@misc{SpeedTree,
author = {SpeedTree},
title = {{SpeedTree Vegetation Modeling}},
url = {https://store.speedtree.com/},
urldate = {2018-05-11}
}
@article{Hendrikx2013,
abstract = {Hundreds of millions of people play computer games every day. For them, game content—from 3D objects to abstract puzzles— plays a major entertainment role. Manual labor has so far ensured that the quality and quantity of game content matched the demands of the playing community, but is facing new scalability challenges due to the exponential growth over the last decade of both the gamer population and the production costs. Procedural Content Generation for Games (PCG-G) may address these challenges by automating, or aiding in, game content generation. PCG-G is difﬁcult, since the generator has to create the content, satisfy constraints imposed by the artist, and return interesting instances for gamers. Despite a large body of research focusing on PCG-G, particularly over the past decade, ours is the ﬁrst comprehensive survey of the ﬁeld of PCG-G. We ﬁrst introduce a comprehensive, six-layered taxonomy of game content: bits, space, systems, scenarios, design, and derived. Second, we survey the methods used across the whole ﬁeld of PCG-G from a large research body. Third, we map PCG-G methods to game content layers; it turns out that many of the methods used to generate game content from one layer can be used to generate content from another. We also survey the use of methods in practice, that is, in commercial or prototype games. Fourth and last, we discuss several directions for future research in PCG-G, which we believe deserve close attention in the near future.},
archivePrefix = {arXiv},
arxivId = {1005.3014},
author = {Hendrikx, Mark and Meijer, Sebastiaan and {Van Der Velden}, Joeri and Iosup, Alexandru},
doi = {10.1145/2422956.2422957},
eprint = {1005.3014},
file = {::},
isbn = {9781627480031},
issn = {15516857},
journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
keywords = {Game content generation,procedural,survey},
month = {feb},
number = {1},
pages = {1--22},
pmid = {1000285845},
publisher = {ACM},
title = {{Procedural content generation for games}},
url = {http://dl.acm.org/citation.cfm?doid=2422956.2422957},
volume = {9},
year = {2013}
}
@article{VanderLinden2014,
abstract = {The use of procedural content generation (PCG) techniques in game development has been mostly restricted to very specific types of game elements. PCG has seldom been deployed for generating entire game levels, a notable exception to this being dungeons: a specific type of game level often encountered in adventure and role playing games. Due to their peculiar combination of pace, gameplay, and game spaces, dungeon levels are among the most suited to showcase the benefits of PCG. This paper surveys research on procedural methods to generate dungeon game levels. We summarize common practices, discuss pros and cons of different approaches, and identify a few promising challenges ahead. In general, what current procedural dungeon generation methods are missing is not performance, but more powerful, accurate, and richer control over the generation process. Recent research results seem to indicate that gameplay-related criteria can provide this high-level control. However, this area is still in its infancy, and many research challenges still lie ahead, e.g., improving the intuitiveness and accessibility of such methods for designers. We also observe that more research is needed into generic mechanisms for automating the generation of the actual dungeon-geometric models. We conclude that the foundations for enabling gameplay-based control of dungeon-level generation are worth being researched, and that its promising results may be instrumental in bringing PCG into mainstream game development.},
author = {{Van Der Linden}, Roland and Lopes, Ricardo and Bidarra, Rafael},
doi = {10.1109/TCIAIG.2013.2290371},
isbn = {1943-068X},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Gameplay semantics,procedural content generation,procedural level generation,role playing games},
month = {mar},
number = {1},
pages = {78--89},
title = {{Procedural generation of dungeons}},
url = {http://ieeexplore.ieee.org/document/6661386/},
volume = {6},
year = {2014}
}
@inproceedings{Mawhorter2010,
abstract = {Existing approaches to procedural level generation in 2D platformer games are, with some notable exceptions, procedures designed to do the work of a human game designer. They offer the usual benefits and disadvantages of AI applied to a cognitive task: they can work much faster than a human level designer, and are in some cases able to explore the design space automatically to find levels with desirable qualities. But they aren't able to capture the human creativity that produces the most interesting level designs, and they are usually very specific to their particular domain. This paper introduces occupancy-regulated extension (ORE), a general geometry assembly algorithm that supports human-design-based level authoring at arbitrary scales.},
author = {Mawhorter, Peter and Mateas, Michael},
booktitle = {Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games, CIG2010},
doi = {10.1109/ITW.2010.5593333},
isbn = {9781424462971},
month = {aug},
pages = {351--358},
publisher = {IEEE},
title = {{Procedural level generation using occupancy-regulated extension}},
url = {http://ieeexplore.ieee.org/document/5593333/},
year = {2010}
}
@misc{AdamGeitgey2016,
abstract = {The world's easiest introduction to Machine Learning},
author = {Geitgey, Adam},
booktitle = {Medium},
title = {{Machine Learning is Fun! – Adam Geitgey}},
url = {https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471{\%}5Cnhttps://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471{\#}.mjhflc9i6},
urldate = {2018-04-29},
year = {2014}
}
@article{Greff,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {::},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
url = {https://arxiv.org/pdf/1503.04069.pdf},
volume = {28},
year = {2017}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {::},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@misc{Olah2015,
abstract = {Humans don't start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don't throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can't do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It's unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. Recurrent Neural Networks have loops. In the above diagram, a chunk of neural network, , looks at some input and outputs a value . A loop allows information to be passed from one step of the network to the next. These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren't all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop: An unrolled recurrent neural network. This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They're the natural architecture of neural network to use for such data. And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning{\ldots} The list goes on. I'll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy's excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sofiyanti, Nery and Fitmawati, Dyah Iriani and Roza, Andesba A.},
booktitle = {GITHUB colah blog},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {9780874216561},
issn = {10282092},
keywords = {Blechnaceae,Indonesia,New species,Stenochlaena},
number = {2},
pages = {137--141},
pmid = {15003161},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2018-04-23},
volume = {22},
year = {2015}
}
@misc{MichaelCopeland2016,
abstract = {The easiest way to think of their relationship is to visualize them as concentric circles with AI — the idea that came first — the largest, then machine learning — which blossomed later, and finally deep learning — which is driving today's AI explosion — fitting inside both.},
author = {{Michael Copeland(Nvidia)}},
booktitle = {Nvidia},
title = {{The Difference Between AI, Machine Learning, and Deep Learning? | NVIDIA Blog}},
url = {https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/},
urldate = {2018-04-16},
year = {2016}
}
